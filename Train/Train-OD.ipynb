{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5125ff-0df4-4e8b-8fc4-c4795f04ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from glob import glob\n",
    "import os, sys, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "import utils\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# Путь к данным\n",
    "images_path = './train/images'\n",
    "annotations_path = './train/annotations'\n",
    "\n",
    "# Определяем количество классов: 1 (фон) + 3 (Text, Math, Image) = 4\n",
    "num_classes = 4\n",
    "\n",
    "# Словарь сопоставления имён классов индексам\n",
    "label_map = {\n",
    "    \"Text\": 1,\n",
    "    \"Math\": 2,\n",
    "    \"Image\": 3\n",
    "}\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, transforms=None):\n",
    "        self.images_path = images_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "        annotations = glob(os.path.join(self.annotations_path, '*.xml'))\n",
    "        self.data = self._load_annotations(annotations)\n",
    "\n",
    "    def _load_annotations(self, annotations):\n",
    "        data = []\n",
    "        for file in annotations:\n",
    "            filename = os.path.basename(file).replace('.xml', '.jpg')\n",
    "            parsedXML = ET.parse(file)\n",
    "            for node in parsedXML.getroot().iter('object'):\n",
    "                obj_class = node.find('name').text\n",
    "                xmin = int(node.find('bndbox/xmin').text)\n",
    "                xmax = int(node.find('bndbox/xmax').text)\n",
    "                ymin = int(node.find('bndbox/ymin').text)\n",
    "                ymax = int(node.find('bndbox/ymax').text)\n",
    "                data.append([filename, obj_class, xmin, xmax, ymin, ymax])\n",
    "        return pd.DataFrame(data, columns=['filename', 'fields', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.data[self.data['filename'] == self.data.iloc[idx]['filename']]\n",
    "        img_path = os.path.join(self.images_path, record['filename'].iloc[0])\n",
    "    \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.data))\n",
    "    \n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "    \n",
    "        boxes = []\n",
    "        labels = []\n",
    "    \n",
    "        for _, row in record.iterrows():\n",
    "            boxes.append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])\n",
    "            class_name = row['fields']\n",
    "            labels.append(label_map.get(class_name, 0))\n",
    "    \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "    \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "    \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = T.ToTensor()(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomVerticalFlip:\n",
    "    def __init__(self, p=0.2):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = T.functional.vflip(image)\n",
    "            if \"boxes\" in target:\n",
    "                bbox = target[\"boxes\"]\n",
    "                # Обновляем координаты ограничивающих прямоугольников\n",
    "                # Высота изображения:\n",
    "                height = image.shape[1]\n",
    "                bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n",
    "                target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [ToTensor()]\n",
    "    if train:\n",
    "        transforms.append(RandomVerticalFlip(0.2))\n",
    "    return Compose(transforms)\n",
    "\n",
    "\n",
    "# Загружаем предобученную модель\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "# Создаём датасет и делим его на train и test\n",
    "full_dataset = VOCDataset(images_path, annotations_path, get_transform(train=True))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=utils.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=utils.collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        return checkpoint['epoch']\n",
    "    else:\n",
    "        print(f\"Контрольная точка по пути {path} не найдена!\")\n",
    "        return 0\n",
    "\n",
    "def plot_metrics(metrics, metric_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(metrics, label=metric_name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f\"{metric_name} Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "losses = []\n",
    "classifier_losses = []\n",
    "box_reg_losses = []\n",
    "objectness_losses = []\n",
    "rpn_box_reg_losses = []\n",
    "\n",
    "num_epochs = 10\n",
    "save_path = './model_checkpoint.pth'\n",
    "start_epoch = load_checkpoint(save_path, model, optimizer, lr_scheduler)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    metric_logger = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=100)\n",
    "\n",
    "    epoch_losses = metric_logger.meters[\"loss\"].global_avg\n",
    "    epoch_loss_classifier = metric_logger.meters[\"loss_classifier\"].global_avg\n",
    "    epoch_loss_box_reg = metric_logger.meters[\"loss_box_reg\"].global_avg\n",
    "    epoch_loss_objectness = metric_logger.meters[\"loss_objectness\"].global_avg\n",
    "    epoch_loss_rpn_box_reg = metric_logger.meters[\"loss_rpn_box_reg\"].global_avg\n",
    "\n",
    "    losses.append(epoch_losses)\n",
    "    classifier_losses.append(epoch_loss_classifier)\n",
    "    box_reg_losses.append(epoch_loss_box_reg)\n",
    "    objectness_losses.append(epoch_loss_objectness)\n",
    "    rpn_box_reg_losses.append(epoch_loss_rpn_box_reg)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    save_checkpoint(epoch, model, optimizer, lr_scheduler, save_path)\n",
    "\n",
    "def save_trained_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "trained_model_path = './model_checkpoint.pth'\n",
    "save_trained_model(model, trained_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
