{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc66c12-50fe-4ded-9c1e-2a0371bf3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#старый код с доработками\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "import Augmentor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, BatchNorm2d, LeakyReLU\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DIR = './' \n",
    "PATH_TEST_DIR = './test/'\n",
    "PATH_TEST_LABELS =  './test.tsv'\n",
    "PATH_TRAIN_DIR =  './train/'\n",
    "PATH_TRAIN_LABELS =  './train.tsv'\n",
    "PATH_VAL_DIR = './fields_val/'\n",
    "PATH_VAL_LABELS = './fields_val.tsv'\n",
    "PREDICT_PATH = \"./test/\"\n",
    "CHECKPOINT_PATH = DIR\n",
    "WEIGHTS_PATH =  \"./ocr_transformer.pth\"\n",
    "PATH_TEST_RESULTS = DIR+'test_result.tsv'\n",
    "TRAIN_LOG = DIR+'train_log.tsv'\n",
    "\n",
    "\n",
    "\n",
    "MODEL = 'modelMAthgOcr'\n",
    "HIDDEN = 512\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "N_HEADS = 4\n",
    "LENGTH = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7ae2c8-4fe3-4d75-9386-c1850b89627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алфавит\n",
    "ALPHABET = ['PAD', 'SOS', 'EOS', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '«', '»', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И',\n",
    "    'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е',\n",
    "    'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё',\n",
    "    '+', '-', '=', '(', ')', '[', ']', '{', '}', '\\\\', '/', '|', '<', '>', '.', ',', ':', '!', '?', '&', '%', '$', '#', '@', '*', '~', '`',\n",
    "    '^', '_']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2ac836-5952-4e00-947a-2129229a68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT = 0.2\n",
    "N_EPOCHS = 128\n",
    "CHECKPOINT_FREQ = 10 \n",
    "DEVICE = 'cuda:0' \n",
    "RANDOM_SEED = 42\n",
    "SCHUDULER_ON = True \n",
    "PATIENCE = 5 \n",
    "OPTIMIZER_NAME = 'Adam' \n",
    "LR = 2e-6\n",
    "\n",
    "\n",
    "CASE = False \n",
    "PUNCT = False \n",
    "\n",
    "\n",
    "WIDTH = 256\n",
    "HEIGHT = 64\n",
    "CHANNELS = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d35d41d-bbaa-42b8-b210-d066ad51bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.scale = torch.nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x) \n",
    "\n",
    "    \n",
    "\n",
    "def process_data(image_dir, labels_dir, ignore=[]):\n",
    "    chars = []\n",
    "    img2label = dict()\n",
    "\n",
    "    raw = open(labels_dir, 'r', encoding='utf-8').read()\n",
    "    temp = raw.split('\\n')\n",
    "    for t in temp:\n",
    "        try:\n",
    "            x = t.split('\\t')\n",
    "            flag = False\n",
    "            for item in ignore:\n",
    "                if item in x[1]:\n",
    "                    flag = True\n",
    "            if flag == False:\n",
    "                img2label[image_dir + x[0]] = x[1]\n",
    "                for char in x[1]:\n",
    "                    if char not in chars:\n",
    "                        chars.append(char)\n",
    "        except:\n",
    "            print('ValueError:', x)\n",
    "            pass\n",
    "\n",
    "    all_labels = sorted(list(set(list(img2label.values()))))\n",
    "    chars.sort()\n",
    "    chars = ['PAD', 'SOS'] + chars + ['EOS']\n",
    "\n",
    "    return img2label, chars, all_labels\n",
    "\n",
    "\n",
    "# TRANSLATE INDICIES TO TEXT\n",
    "def indicies_to_text(indexes, idx2char):\n",
    "    text = \"\".join([idx2char[i] for i in indexes])\n",
    "    text = text.replace('EOS', '').replace('PAD', '').replace('SOS', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def char_error_rate(p_seq1, p_seq2):\n",
    "    p_vocab = set(p_seq1 + p_seq2)\n",
    "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
    "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
    "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
    "    return editdistance.eval(''.join(c_seq1),\n",
    "                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))\n",
    "\n",
    "\n",
    "\n",
    "def process_image(img):\n",
    "    w, h, _ = img.shape\n",
    "    new_w = HEIGHT\n",
    "    new_h = int(h * (new_w / w))\n",
    "    img = cv2.resize(img, (new_h, new_w))\n",
    "    w, h, _ = img.shape\n",
    "\n",
    "    img = img.astype('float32')\n",
    "\n",
    "    new_h = WIDTH\n",
    "    if h < new_h:\n",
    "        add_zeros = np.full((w, new_h - h, 3), 255)\n",
    "        img = np.concatenate((img, add_zeros), axis=1)\n",
    "\n",
    "    if h > new_h:\n",
    "        img = cv2.resize(img, (new_h, new_w))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_data(img_paths):\n",
    "    data_images = []\n",
    "    for path in tqdm(img_paths):\n",
    "        img = np.asarray(Image.open(path).convert('RGB'))\n",
    "        try:\n",
    "            img = process_image(img)\n",
    "            data_images.append(img.astype('uint8'))\n",
    "        except:\n",
    "            print(path)\n",
    "            img = process_image(img)\n",
    "    return data_images\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, loader, case=True, punct=True):\n",
    "    model.eval()\n",
    "    metrics = {'loss': 0, 'wer': 0, 'cer': 0}\n",
    "    result = {'true': [], 'predicted': [], 'wer': []}\n",
    "    with torch.no_grad():\n",
    "        for (src, trg) in loader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            logits = model(src, trg[:-1, :])\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
    "            out_indexes = model.predict(src)\n",
    "            \n",
    "            true_phrases = [indicies_to_text(trg.T[i][1:], ALPHABET) for i in range(BATCH_SIZE)]\n",
    "            pred_phrases = [indicies_to_text(out_indexes[i], ALPHABET) for i in range(BATCH_SIZE)]\n",
    "            \n",
    "            if not case:\n",
    "                true_phrases = [phrase.lower() for phrase in true_phrases]\n",
    "                pred_phrases = [phrase.lower() for phrase in pred_phrases]\n",
    "            if not punct:\n",
    "                true_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
    "                                for phrase in true_phrases]\n",
    "                pred_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
    "                                for phrase in pred_phrases]\n",
    "            \n",
    "            metrics['loss'] += loss.item()\n",
    "            metrics['cer'] += sum([char_error_rate(true_phrases[i], pred_phrases[i]) \\\n",
    "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
    "            metrics['wer'] += sum([int(true_phrases[i] != pred_phrases[i]) \\\n",
    "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
    "\n",
    "            for i in range(len(true_phrases)):\n",
    "                result['true'].append(true_phrases[i])\n",
    "                result['predicted'].append(pred_phrases[i])\n",
    "                result['wer'].append(char_error_rate(true_phrases[i], pred_phrases[i]))\n",
    "\n",
    "    for key in metrics.keys():\n",
    "        metrics[key] /= len(loader)\n",
    "\n",
    "    return metrics, result\n",
    "\n",
    "\n",
    "def prediction(model, test_dir, char2idx, idx2char):\n",
    "    preds = {}\n",
    "    os.makedirs('/output', exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for filename in os.listdir(test_dir):\n",
    "            img = Image.open(test_dir + filename).convert('RGB')\n",
    "\n",
    "            img = process_image(np.asarray(img)).astype('uint8')\n",
    "            img = img / img.max()\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "            src = torch.FloatTensor(img).unsqueeze(0).to(DEVICE)\n",
    "            if CHANNELS == 1:\n",
    "                src = transforms.Grayscale(CHANNELS)(src)\n",
    "            out_indexes = model.predict(src)\n",
    "            pred = indicies_to_text(out_indexes[0], idx2char)\n",
    "            preds[filename] = pred\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, X_type=None, Y_type=None):\n",
    "        self.X_type = X_type\n",
    "\n",
    "    def __call__(self, X):\n",
    "        X = X.transpose((2, 0, 1))\n",
    "        X = torch.from_numpy(X)\n",
    "        if self.X_type is not None:\n",
    "            X = X.type(self.X_type)\n",
    "        return X\n",
    "\n",
    "\n",
    "def log_config(model):\n",
    "    print('transformer layers: {}'.format(model.enc_layers))\n",
    "    print('transformer heads: {}'.format(model.transformer.nhead))\n",
    "    print('hidden dim: {}'.format(model.decoder.embedding_dim))\n",
    "    print('num classes: {}'.format(model.decoder.num_embeddings))\n",
    "    print('backbone: {}'.format(model.backbone_name))\n",
    "    print('dropout: {}'.format(model.pos_encoder.dropout.p))\n",
    "    print(f'{count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "def log_metrics(metrics, path_to_logs=None):\n",
    "    if path_to_logs != None:\n",
    "        f = open(path_to_logs, 'a')\n",
    "    if metrics['epoch'] == 1:\n",
    "        if path_to_logs != None:\n",
    "            f.write('Epoch\\tTrain_loss\\tValid_loss\\tCER\\tWER\\tTime\\n')\n",
    "        print('Epoch   Train_loss   Valid_loss   CER   WER    Time    LR')\n",
    "        print('-----   -----------  ----------   ---   ---    ----    ---')\n",
    "    print('{:02d}       {:.2f}         {:.2f}       {:.2f}   {:.2f}   {:.2f}   {:.7f}'.format(\\\n",
    "        metrics['epoch'], metrics['train_loss'], metrics['loss'], metrics['cer'], \\\n",
    "        metrics['wer'], metrics['time'], metrics['lr']))\n",
    "    if path_to_logs != None:\n",
    "        f.write(str(metrics['epoch'])+'\\t'+str(metrics['train_loss'])+'\\t'+str(metrics['loss'])+'\\t'+str(metrics['cer'])+'\\t'+str(metrics['wer'])+'\\t'+str(metrics['time'])+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "\n",
    "# plot images\n",
    "def show_img_grid(images, labels, N):\n",
    "    n = int(N**(0.5))\n",
    "    k = 0\n",
    "    f, axarr = plt.subplots(n,n,figsize=(10,10))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            axarr[i,j].set_title(labels[k])\n",
    "            axarr[i,j].imshow(images[k])\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003bf4d4-3941-4740-9d94-7c35c00551bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_labels(s, char2idx):\n",
    "    return [char2idx['SOS']] + [char2idx[i] for i in s if i in char2idx.keys()] + [char2idx['EOS']]\n",
    "\n",
    "class TextLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_name, labels, transforms, char2idx, idx2char):\n",
    "        self.images_name = images_name\n",
    "        self.labels = labels\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.transform = transforms\n",
    "\n",
    "    def get_info(self):\n",
    "        N = len(self.labels)\n",
    "        max_len = max(len(label) for label in self.labels)\n",
    "        \n",
    "        all_chars = ''.join(self.labels)\n",
    "        counter = Counter(all_chars)\n",
    "        counter = dict(sorted(counter.items(), key=lambda item: item[1]))\n",
    "\n",
    "        most_common_char = list(counter.items())[-1]\n",
    "        least_common_char = list(counter.items())[0]\n",
    "        \n",
    "        print(\n",
    "            f\"Size of dataset: {N}\\n\"\n",
    "            f\"Max length of expression: {max_len}\\n\"\n",
    "            f\"The most common char: {most_common_char}\\n\"\n",
    "            f\"The least common char: {least_common_char}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images_name[index]\n",
    "        img = self.transform(img)\n",
    "        img = img / img.max()\n",
    "        img = img ** (random.random() * 0.7 + 0.6)\n",
    "\n",
    "        label = text_to_labels(self.labels[index], self.char2idx)\n",
    "        return (torch.FloatTensor(img), torch.LongTensor(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TextCollate:\n",
    "    def __call__(self, batch):\n",
    "        max_len = max([len(item[1]) for item in batch]) \n",
    "        x_padded = []\n",
    "        y_padded = torch.zeros(max_len, len(batch)).long() \n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            x_padded.append(batch[i][0].unsqueeze(0))  \n",
    "            y = batch[i][1]  \n",
    "            y_padded[:len(y), i] = y  \n",
    "\n",
    "        x_padded = torch.cat(x_padded)  \n",
    "        return x_padded, y_padded\n",
    "\n",
    "\n",
    "p = Augmentor.Pipeline()\n",
    "p.shear(max_shear_left=2, max_shear_right=2, probability=0.7)\n",
    "p.random_distortion(probability=1.0, grid_width=3, grid_height=3, magnitude=11)\n",
    "\n",
    "TRAIN_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(CHANNELS),\n",
    "    p.torch_transform(), \n",
    "    transforms.ColorJitter(contrast=(0.5, 1), saturation=(0.5, 1)),\n",
    "    transforms.RandomRotation(degrees=(-9, 9)),\n",
    "    transforms.RandomAffine(10, None, [0.6, 1], 3, fill=255),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "TEST_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(CHANNELS),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6728ad6-8f1d-446a-a53f-6e28933453be",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_TRANSFORMS = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(CHANNELS),\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fee3443-f348-494a-a9d9-92991be1ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.backbone_name = 'conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)'\n",
    "\n",
    "        self.conv0 = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
    "        self.conv3 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
    "        self.conv5 = Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv6 = Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n",
    "        \n",
    "        self.pool1 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.pool3 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
    "\n",
    "        self.bn0 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn1 = BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn2 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn3 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn4 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn5 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn6 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        self.activ = LeakyReLU()\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
    "        self.decoder = nn.Embedding(outtoken, hidden)\n",
    "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
    "        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n",
    "                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden, outtoken)\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.memory_mask = None\n",
    "        \n",
    "        log_config(self)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=DEVICE), 1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "    \n",
    "    def _get_features(self, src):\n",
    "        x = self.activ(self.bn0(self.conv0(src)))\n",
    "        x = self.pool1(self.activ(self.bn1(self.conv1(x))))\n",
    "        x = self.activ(self.bn2(self.conv2(x)))\n",
    "        x = self.pool3(self.activ(self.bn3(self.conv3(x))))\n",
    "        x = self.activ(self.bn4(self.conv4(x)))\n",
    "        x = self.pool5(self.activ(self.bn5(self.conv5(x))))\n",
    "        x = self.activ(self.bn6(self.conv6(x)))\n",
    "        x = x.permute(0, 3, 1, 2).flatten(2).permute(1, 0, 2)\n",
    "        return x\n",
    "\n",
    "    def predict(self, batch):\n",
    "        result = []\n",
    "        for item in batch:\n",
    "            x = self._get_features(item.unsqueeze(0))\n",
    "            memory = self.transformer.encoder(self.pos_encoder(x))\n",
    "            out_indexes = [ALPHABET.index('SOS'), ]\n",
    "            for i in range(100):\n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(DEVICE)\n",
    "                output = self.fc_out(self.transformer.decoder(self.pos_decoder(self.decoder(trg_tensor)), memory))\n",
    "\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == ALPHABET.index('EOS'):\n",
    "                    break\n",
    "            result.append(out_indexes)\n",
    "        return result\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device) \n",
    "\n",
    "        x = self._get_features(src)\n",
    "        src_pad_mask = self.make_len_mask(x[:, :, 0])\n",
    "        src = self.pos_encoder(x)\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.pos_decoder(trg)\n",
    "\n",
    "        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n",
    "                                  memory_mask=self.memory_mask,\n",
    "                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n",
    "                                  memory_key_padding_mask=src_pad_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8fa78a-fc0e-4423-8a99-782f57d5189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:-1, :])\n",
    "\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def fit(model, optimizer, scheduler, criterion, train_loader, val_loader, start_epoch=0, end_epoch=24):\n",
    "    metrics = []\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        epoch_metrics = {}\n",
    "        start_time = time()\n",
    "        train_loss = train(model, optimizer, criterion, train_loader)\n",
    "        end_time = time()\n",
    "        epoch_metrics, _ = evaluate(model, criterion, val_loader)\n",
    "        epoch_metrics['train_loss'] = train_loss\n",
    "        epoch_metrics['epoch'] = epoch\n",
    "        epoch_metrics['time'] = end_time - start_time\n",
    "        epoch_metrics['lr'] = optimizer.param_groups[0][\"lr\"]\n",
    "        metrics.append(epoch_metrics)\n",
    "        log_metrics(epoch_metrics, TRAIN_LOG)\n",
    "        if scheduler != None:\n",
    "            scheduler.step(epoch_metrics['loss'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5083f0-5d96-41a0-b57c-6e96ab384ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
    "idx2char = {idx: char for idx, char in enumerate(ALPHABET)}\n",
    "\n",
    "print(f\"loading dataset {PATH_TRAIN_DIR} ...\")\n",
    "img2label, _, all_words = process_data(PATH_TRAIN_DIR, PATH_TRAIN_LABELS) \n",
    "img_names, labels = list(img2label.keys()), list(img2label.values())\n",
    "X_train = generate_data(img_names)\n",
    "y_train = labels\n",
    "\n",
    "train_dataset = TextLoader(X_train, y_train, TRAIN_TRANSFORMS, char2idx, idx2char)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n",
    "                                           batch_size=BATCH_SIZE, pin_memory=True,\n",
    "                                           drop_last=True, collate_fn=TextCollate())\n",
    "\n",
    "print(f\"loading dataset {PATH_TEST_DIR} ...\")\n",
    "img2label, _, all_words = process_data(PATH_TEST_DIR, PATH_TEST_LABELS) \n",
    "img_names, labels = list(img2label.keys()), list(img2label.values())\n",
    "X_test = generate_data(img_names)\n",
    "y_test = labels\n",
    "\n",
    "test_dataset = TextLoader(X_test, y_test, TEST_TRANSFORMS, char2idx ,idx2char)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True,\n",
    "                                           batch_size=BATCH_SIZE, pin_memory=True,\n",
    "                                           drop_last=True, collate_fn=TextCollate())\n",
    "\n",
    "print(\"TRAIN DATASET:\")\n",
    "train_dataset.get_info()\n",
    "print(\"\\nTEST DATASET:\")\n",
    "test_dataset.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc6dda-692d-4628-811b-dbabe2fa1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
    "idx2char = {idx: char for idx, char in enumerate(ALPHABET)}\n",
    "\n",
    "print(f\"loading dataset {PATH_VAL_DIR} ...\")\n",
    "img2label, _, all_words = process_data(PATH_VAL_DIR, PATH_VAL_LABELS) \n",
    "img_names, labels = list(img2label.keys()), list(img2label.values())\n",
    "X_val = generate_data(img_names)\n",
    "y_val = labels\n",
    "\n",
    "val_dataset = TextLoader(X_val, y_val, VAL_TRANSFORMS, char2idx ,idx2char)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=True,\n",
    "                                           batch_size=BATCH_SIZE, pin_memory=True,\n",
    "                                           drop_last=True, collate_fn=TextCollate())\n",
    "print(\"VAL DATASET:\")\n",
    "val_dataset.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04436642-5a86-41c2-a93f-c06a3c278d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n",
    "                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\n",
    "optimizer = torch.optim.__getattribute__(OPTIMIZER_NAME)(model.parameters(), lr=LR)\n",
    "\n",
    "if SCHUDULER_ON:\n",
    "    scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=PATIENCE)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "print(f'checkpoints are saved in {CHECKPOINT_PATH} every {CHECKPOINT_FREQ} epochs')\n",
    "\n",
    "# Обучение \n",
    "for epoch in range(1, N_EPOCHS, CHECKPOINT_FREQ):\n",
    "     fit(model, optimizer, scheduler, criterion, train_loader, test_loader, epoch, epoch+CHECKPOINT_FREQ)\n",
    "     torch.save(model.state_dict(), CHECKPOINT_PATH+'checkpoint_{}.pt'.format(epoch+CHECKPOINT_FREQ))\n",
    "\n",
    "def save_trained_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "trained_model_path = './ocr_transformer.pth'\n",
    "\n",
    "save_trained_model(model, trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a518d9-29dd-4aaa-8ddb-fa1adc00cf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
